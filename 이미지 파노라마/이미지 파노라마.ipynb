{
  "cells": [
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import cv2\nimport numpy as np",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dim=(768,768)\nleft=cv2.imread('img/restaurant1.jpg',cv2.IMREAD_COLOR)\nleft=cv2.resize(left,dim,interpolation = cv2.INTER_AREA)  \nright=cv2.imread('img/restaurant2.jpg',cv2.IMREAD_COLOR)\nright=cv2.resize(right,dim,interpolation = cv2.INTER_AREA) \n\nimages=[]\nimages.append(left)\nimages.append(right)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "stitcher = cv2.createStitcher()\n\nret,pano = stitcher.stitch(images)\n\nif ret==cv2.STITCHER_OK:\n    cv2.imshow('Panorama',pano)\n    cv2.waitKey()\n    cv2.destroyAllWindows()",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import cv2\nimport numpy as np\n\nimg1 = cv2.imread('img/restaurant1.jpg')\nimg2 = cv2.imread('img/restaurant2.jpg')\ngray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\n# ORB, BF-Hamming 로 knnMatch  ---①\ndetector = cv2.ORB_create()\nkp1, desc1 = detector.detectAndCompute(gray1, None)\nkp2, desc2 = detector.detectAndCompute(gray2, None)\n\nmatcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\nmatches = matcher.knnMatch(desc1, desc2, 2)\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# 이웃 거리의 75%로 좋은 매칭점 추출---②\nratio = 0.75\ngood_matches = [first for first,second in matches \\\n                    if first.distance < second.distance * ratio]\nprint('good matches:%d/%d' %(len(good_matches),len(matches)))",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "good matches:68/500\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n# 좋은 매칭점의 queryIdx로 원본 영상의 좌표 구하기 ---③\nsrc_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\ndst_pts = np.float32([ kp2[m.queryIdx].pt for m in good_matches ])\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n#위의 코드가 이해 안되면 출력해보기\n#print(kp1[m.queryIdx]) #객체\n#print(kp1[m.queryIdx].pt) #좌표\n    \n# 원근 변환 행렬 구하기 ---⑤\nmtrx, mask = cv2.findHomography( dst_pts,src_pts)\n\n# 원본 영상 크기로 변환 영역 좌표 생성 ---⑥\nhl, wl = img1.shape[:2]\nhr, wr = img2.shape[:2]\nw = wl + wr \nh = hl + hr",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "w",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "1280"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n# h,w, = img1.shape[:2]\npts = np.float32([ [[0,0]],[[0,h-1]],[[wl-1,hl-1]],[[wl-1,0]] ])\n\n# 원본 영상 좌표를 원근 변환  ---⑦\ndst = cv2.perspectiveTransform(pts,mtrx)\n# # 변환 좌표 영역을 대상 영상에 그리기 ---⑧\n#img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n\n\n#원근 변환 행렬로 오른쪽 사진을 원근 변환, 결과 이미지 크기는 사진 2장 크기\ndst = cv2.warpPerspective(img2, mtrx, (wr, hr))\n\n# 왼쪽 사진을 원근 변환한 왼쪽 영역에 합성\ndst[0:hl, 0:wl] = img1\n\n# # 좋은 매칭 그려서 출력 ---⑨\nres = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n                     flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n\n\n\ncv2.imshow(\"Panorama\", res)\ncv2.waitKey(0)                        \ncv2.destroyAllWindows()",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "cv2.imshow('a',dst)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import cv2, numpy as np\nimport matplotlib.pyplot as plt\n# 1. 이미지 입력 및 그레이스케일 변환\nimg1 = cv2.imread('img/restaurant1.jpg')                    # img1 입력\nimg2 = cv2.imread('img/restaurant2.jpg')                    # img2 입력\ngray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)              # img1 gray 변환\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)              # img2 gray 변환\n\n# 2. ORB, BF-Hamming 로 knnMatch\ndetector = cv2.ORB_create()                                 # ORB 검출기 생성\nkp1, desc1 = detector.detectAndCompute(gray1, None)         # gray1의 키포인트와 디스크립터 검출\nkp2, desc2 = detector.detectAndCompute(gray2, None)         # gray2의 키포인트와 디스크립터 검출\nmatcher = cv2.BFMatcher(cv2.NORM_HAMMING2)                  # BF-Hamming 매칭 생성\nmatches = matcher.knnMatch(desc1, desc2, 2)                 # knnMatch, k=2(2번째 이웃 거리)\n\n# 3. 좋은 매칭점 추출\nratio = 0.65                                                #이웃 거리의 75%로(65~75%)\ngood_matches = [first for first,second in matches \\\n                    if first.distance < second.distance * ratio]\nprint('good matches:%d/%d' %(len(good_matches),len(matches)))\n\n# 4. 좋은 매칭점의 queryIdx로 원본 영상의 좌표 구하기\nsrc_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\ndst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n\nmtrx, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 4.0)\n\nhl, wl, = img1.shape[:2]\nhr, wr, = img2.shape[:2]\n\nresult = cv2.warpPerspective(img2, mtrx, (wr+wl, hr))\nresult[0:hl, 0:wl] = img1\n\nplt.figure(figsize=(20,20)),plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB)),plt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}