{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. stitcher 클래스 사용 ( OpenCV 제공)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=(768,768)\n",
    "left=cv2.imread('img/restaurant1.jpg',cv2.IMREAD_COLOR)\n",
    "left=cv2.resize(left,dim,interpolation = cv2.INTER_AREA)  \n",
    "right=cv2.imread('img/restaurant2.jpg',cv2.IMREAD_COLOR)\n",
    "right=cv2.resize(right,dim,interpolation = cv2.INTER_AREA) \n",
    "\n",
    "images=[]\n",
    "images.append(left)\n",
    "images.append(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = cv2.createStitcher()\n",
    "\n",
    "ret,pano = stitcher.stitch(images)\n",
    "\n",
    "if ret==cv2.STITCHER_OK:\n",
    "    cv2.imshow('Panorama',pano)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 매칭점 찾아 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/restaurant1.jpg')\n",
    "img2 = cv2.imread('img/restaurant2.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB, BF-Hamming 로 knnMatch  ---①\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\n",
    "matches = matcher.knnMatch(desc1, desc2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good matches:68/500\n"
     ]
    }
   ],
   "source": [
    "# 이웃 거리의 75%로 좋은 매칭점 추출---②\n",
    "ratio = 0.75\n",
    "good_matches = [first for first,second in matches \\\n",
    "                    if first.distance < second.distance * ratio]\n",
    "print('good matches:%d/%d' %(len(good_matches),len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 좋은 매칭점의 queryIdx로 원본 영상의 좌표 구하기 ---③\n",
    "src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "dst_pts = np.float32([ kp2[m.queryIdx].pt for m in good_matches ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#위의 코드가 이해 안되면 출력해보기\n",
    "#print(kp1[m.queryIdx]) #객체\n",
    "#print(kp1[m.queryIdx].pt) #좌표\n",
    "    \n",
    "# 원근 변환 행렬 구하기 ---⑤\n",
    "mtrx, mask = cv2.findHomography( dst_pts,src_pts)\n",
    "\n",
    "# 원본 영상 크기로 변환 영역 좌표 생성 ---⑥\n",
    "hl, wl = img1.shape[:2]\n",
    "hr, wr = img2.shape[:2]\n",
    "w = wl + wr \n",
    "h = hl + hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# h,w, = img1.shape[:2]\n",
    "pts = np.float32([ [[0,0]],[[0,h-1]],[[wl-1,hl-1]],[[wl-1,0]] ])\n",
    "\n",
    "# 원본 영상 좌표를 원근 변환  ---⑦\n",
    "dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "# # 변환 좌표 영역을 대상 영상에 그리기 ---⑧\n",
    "#img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "#원근 변환 행렬로 오른쪽 사진을 원근 변환, 결과 이미지 크기는 사진 2장 크기\n",
    "dst = cv2.warpPerspective(img2, mtrx, (wr, hr))\n",
    "\n",
    "# 왼쪽 사진을 원근 변환한 왼쪽 영역에 합성\n",
    "dst[0:hl, 0:wl] = img1\n",
    "\n",
    "# # 좋은 매칭 그려서 출력 ---⑨\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                     flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow(\"Panorama\", res)\n",
    "cv2.waitKey(0)                        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('a',dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 1. 이미지 입력 및 그레이스케일 변환\n",
    "img1 = cv2.imread('img/restaurant1.jpg')                    # img1 입력\n",
    "img2 = cv2.imread('img/restaurant2.jpg')                    # img2 입력\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)              # img1 gray 변환\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)              # img2 gray 변환\n",
    "\n",
    "# 2. ORB, BF-Hamming 로 knnMatch\n",
    "detector = cv2.ORB_create()                                 # ORB 검출기 생성\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)         # gray1의 키포인트와 디스크립터 검출\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)         # gray2의 키포인트와 디스크립터 검출\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)                  # BF-Hamming 매칭 생성\n",
    "matches = matcher.knnMatch(desc1, desc2, 2)                 # knnMatch, k=2(2번째 이웃 거리)\n",
    "\n",
    "# 3. 좋은 매칭점 추출\n",
    "ratio = 0.65                                                #이웃 거리의 75%로(65~75%)\n",
    "good_matches = [first for first,second in matches \\\n",
    "                    if first.distance < second.distance * ratio]\n",
    "print('good matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "\n",
    "# 4. 좋은 매칭점의 queryIdx로 원본 영상의 좌표 구하기\n",
    "src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "\n",
    "mtrx, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 4.0)\n",
    "\n",
    "hl, wl, = img1.shape[:2]\n",
    "hr, wr, = img2.shape[:2]\n",
    "\n",
    "result = cv2.warpPerspective(img2, mtrx, (wr+wl, hr))\n",
    "result[0:hl, 0:wl] = img1\n",
    "\n",
    "plt.figure(figsize=(20,20)),plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB)),plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
